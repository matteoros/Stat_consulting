---
title: "Analysis3"
author: "Katie Lindefjeld"
date: "2023-11-29"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Set up file

```{r}
##### Clear global environment and console
rm(list = ls()) #GE
cat("\014") # console

##### Load libraries
library(readxl) 
library(dplyr) 
library(metafor) # package used for meta-analyses
library(ggplot2)
```
## Reading and cleaning the data, check out measure distribution

```{r}
##### Read in papers
df = read_excel("PaperCharacterizationTable_Meta_RAtest.xlsx", sheet = "Metacognition", skip = 1)
df = df %>% select(-starts_with('...'))
head(df)

freq = table(df$"Authors")
df_table <- as.data.frame(table(as.data.frame(freq)[,2]))


##### Measure distribution before we remove anything
p <- ggplot(data = df_table, aes(x = Var1, y = Freq)) +
  geom_bar(stat = "identity", color = 'steelblue2', fill = 'steelblue2') +
  theme_bw() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank()) +
  scale_y_continuous(breaks = seq(0, max(df_table$Freq), by = 2)) +
  labs(x = "# of Metacognition Measures Assessed Per Study", 
       y = "# of Studies Using the Different Amounts of Measures", 
       title = "# of Studies Using the Different Amounts of Metacognition Measures")

p
rm(freq, df_table)
```
### Select only the columns we're interested in:
```{r}
##### Preallocate the dataframe, and rename the columns in 
df_meta = data.frame(matrix(ncol = 13, nrow = nrow(df))) # preallocate the dataframe

colnames(df_meta) = c('Authors', 
                      'Year', # Year of publication 
                      'Age', # Mean age
                      'Clinical', # Clinical vs. recruited participants
                      'Instrument', #  metacognition instrument, e.g. MCQ-C
                      'Instrument_simpl',
                      'Dimensions', # metacognition dimension, e.g. positive beliefs 
                      'Summary_dim', # preallocate for later when we summarize metacognition dims. 
                      'N', # Sample size included (of SAD patients)
                      'd_r', # effect measure type
                      'effect_size', 
                      'V', # variance
                      '95_CI') # 95% confidence interval

##### Select only those relevant columns from the original dataframe
df_meta[,colnames(df_meta)] = df %>% select(`Authors`, `Year of publication`,
                                              `mean age`, `Clinical sample`, `Metacognition trait or state`, `Manipulation`,# moderators
                                              `Dimensions used (if looked at different dimension make separate rows for each dimension)`,
                                              `Origin Country of study`, # intentionally included. will be excluded later
                                              `Sample size included`, `d or R`, `effect size`, `V`, `95% CI`)

df_meta$Dimensions[df_meta$Dimensions == "postive dimension"] = "positive dimension"
##### Indicate which measures are general (are not specifically positive or negative)
df_meta$Summary_dim = df_meta$Dimensions
meta_measure = which(!(df_meta$Summary_dim %in% c("positive dimension", "negative dimension"))) 
df_meta$Summary_dim[meta_measure] = "General"

##### Summarize MCQ measures (since ultimately they're rephrased for children, but they ask the same questions)
df_meta$Instrument_simpl = df_meta$Instrument
df_meta$Instrument_simpl[grep("MCQ", df_meta$Instrument)] <- "MCQ"

rm(meta_measure)
df_meta |> select(Dimensions, Summary_dim)
```

### Clean up paper titles
```{r Author formatting}
##### Two papers have missing commas after the surname
df_meta$Authors[df_meta$Authors == "Dal Bo E. and Gentili C. and Fischmeister F.Ph.S. and Cecchetto C."] = "Dal Bo, E. and Gentili, C. and Fischmeister, F.Ph.S. and Cecchetto, C."
df_meta$Authors[df_meta$Authors == "Mohammadi B. and Beige N.A."] <- "Mohammadi, B. and Beige, N.A."


##### Rename the authors so that the names are abbreviated, and so that each paper has a unique name ID
first_names = sapply(strsplit(df_meta$Authors, ", "), function(x) x[1])
full_names = paste(first_names, "et al.", df_meta$Year, sep = ", ")
df_meta$Authors = full_names


##### Add in A and B to the unique studies with the same authors and year of publication
Hearn = which(df_meta$Authors == "Hearn, et al., 2017")
df_meta$Authors[Hearn[1:2]] = paste(df_meta$Authors[Hearn[1]], "A")
df_meta$Authors[Hearn[3:4]] = paste(df_meta$Authors[Hearn[3]], "B")


##### We define a function so that you can see how many papers remain after removing those with missing information.
papers_left = function(df){sprintf("%.2f papers",length(unique(df_meta$Authors)))} # I changed here df_meta instead of df
papers_left(df_meta) # We start with 26

##### Remove unused variables
rm(first_names, full_names, Hearn)
```

### Standardize all of the input types 
```{r}
##### remove the parentheses from the effect sizes and correct typo (we can see where the decimal goes from the confidence interval)
df_meta$effect_size <- gsub("[()]", "", df_meta$effect_size) # remove ()
typo = which(df_meta$effect_size==1177)
df_meta$effect_size[typo] = 1.177 # correct typo


##### Standardize type per column
# Numeric variables
numeric_columns <- c('Age', 'N', 'effect_size', 'V')
df_meta[, numeric_columns] <- lapply(df_meta[, numeric_columns], as.numeric) # this is giving the warning that is introducing NAs

# Factor variables
factor_columns <- c('Authors', 'Clinical', 'Instrument', 'Instrument_simpl', 'Dimensions', 'Summary_dim', 'd_r')
df_meta[, factor_columns] <- lapply(df_meta[, factor_columns], as.factor)
df_meta$Instrument[df_meta$Instrument == "-"] = NA

rm(numeric_columns,factor_columns, typo) 
```


### Compute standard Error and standardize confidence interval reporting

It is also possible to calculat the confidence interval directly from the estimate and the sd (for effect sizes SE = sd).
```{r}
##### Add in standard error column
df_meta$SE = sqrt(df_meta$V)


##### Standardize confidence intervals. Some entries use comma separator, some use period. We standardize to use a period.
library(stringr)
num_commas = str_count(df_meta$`95_CI`, ',')
ind = which(num_commas == 3)

for(i in ind) {
  string = df_meta$`95_CI`[i]
  string = gsub("^\\[([^,]*),", "[\\1.", string)
  string = gsub(",([^,]*)\\]$", ".\\1]", string)
  df_meta$`95_CI`[i] = string 
}

##### Define function for splitting the confidence intervals into two separate columns so that they can be encoded as numeric
split_values <- function(vec) {
  # Split the string at the comma
  split_vec <- strsplit(vec, ",")
  
  # Initialize vectors for val1 and val2
  val1 <- numeric(length(vec))
  val2 <- numeric(length(vec))
  
  # Loop over the split_vec list
  for (i in seq_along(split_vec)) {
    # Check if there are more than two elements
    if (length(split_vec[[i]]) > 2) {
      # There is an error
      next
    }
    
    # Remove the "[" and "]" characters, replace any commas with periods, and convert to numeric
    val1[i] <- as.numeric(sub(",", ".", gsub("\\[", "", split_vec[[i]][1])))
    val2[i] <- as.numeric(sub(",", ".", gsub("\\]", "", split_vec[[i]][2])))
  }
  
  # Return a data frame with two columns
  return(data.frame(lower_CI = val1, upper_CI = val2))
}

##### Employ function on the data set. lower_CI becomes the lower bound, and upper_CI becomes the upper bound
df = split_values(df_meta$'95_CI') # this is introducing NAs
df_meta[ , c("lower_CI","upper_CI")] = df
df_meta <- df_meta %>% select(-"95_CI") # remove the original variable

rm(i, ind, num_commas, string, df)
df_meta
```
### Remove studies with missing information
Select only if the effect size is available (r or d)
```{r}
##### Find papers which do not have an r or a d measure
no_d_r = which(!(df_meta$d_r %in% c("r", "d"))) 
df_meta[no_d_r,] |> nrow()

##### Find papers which do not have a variance
no_var = which(is.na(df_meta$V))
df_meta[no_var,] |> nrow()

##### Find papers which do not have a mean age
no_age = which(is.na(df_meta$Age))
df_meta[no_age,] |> nrow()

##### Some may have two measures missing, so we want the unique rows to remove:
row_rm = unique(c(no_d_r, # no d or r measure
                  no_var, # no variance
                  no_age))# no median age
length(row_rm)

##### Remove the papers with missing information 
df_meta = df_meta[-row_rm,]
papers_left(df_meta) #number of papers left 

rm(no_age, no_d_r, no_var, row_rm)
df_meta


##### Measure distribution after problematic papers removed
freq = table(df_meta$"Authors")
df_table <- as.data.frame(table(as.data.frame(freq)[,2]))

p1 <- ggplot(data = df_table, aes(x = Var1, y = Freq)) +
  geom_bar(stat = "identity", color = 'steelblue', fill = 'steelblue') +
  theme_bw() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank()) +
  scale_y_continuous(breaks = seq(0, max(df_table$Freq), by = 2)) +
  labs(x = "# of Metacognition Measures Assessed Per Study", 
       y = "# of Studies Using the Different Amounts of Measures", 
       title = "# of Studies Using the Different Amounts of Metacognition Measures")

p1
rm(freq,df_table)
```
## Prepare data for meta-analysis


```{r}
#View(df_meta)
```

### Only selecting the r measurements

```{r}
df_meta_r = df_meta[df_meta$d_r == "r", ]

df_meta_r$effect_size
```

```{r}
df_meta_r = escalc(measure = "ZCOR", ri = effect_size, ni = N,
                   data = df_meta_r, slab = Authors)
```

```{r}
View(df_meta_r)
```


```{r}
result_classic_r = rma(yi, vi, data = df_meta_r, method="DL")
result_classic_r
```
We notice that there is high $I^2$, therefore this approach is wrong! We need to account for the non independence of the measurements coming from the same paper (same sample)!

```{r}
colnames(df_meta_r)

```


```{r}
result_r1 =  rma.mv(yi,vi,
                    # mods = ~ Age + Clinical + Instrument_simpl,
                    slab = Authors,
                    data = df_meta_r,
                    random = ~ 1 | Authors/Summary_dim,
                    test = "t", 
                    method = "REML")
```

```{r}
par(mar=c(4,4,2,2))
forest(result_r1, header = T, mlab = "Summary",
       cex = 0.65)
```

```{r}
funnel(result_r1)
```
This graph is awful! Wrong

```{r}
df_meta_r$Summary_dim
```



```{r}
sigma2 = result_r1$sigma2 
names(sigma2) = c("sigma_squared1", "sigma_squared2")
sigma2
```


Notice the two variance components $\sigma_1^2$ , $\sigma_2^2$, for the between-cluster (Authors) heterogeneity and the within-cluster (type of measurement within Authors) heterogeneity.
 
```{r}
W <- diag(1/result_r1$vi)
X <- model.matrix(result_r1)
P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
I_square = 100 * sum(result_r1$sigma2) / (sum(result_r1$sigma2) + (result_r1$k-result_r1$p)/sum(diag(P)))
I_square
```
 Source: https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate#multilevel_models
 

However, we can also break things down to estimate how much of the total variance can be attributed to between- and within-cluster heterogeneity separately:

```{r}
Separated_I_squared = 100 * result_r1$sigma2 / (sum(result_r1$sigma2) + (result_r1$k-result_r1$p)/sum(diag(P)))
names(Separated_I_squared) = c("between_variance", "within_variance")
Separated_I_squared
```

Between variance = is the variance of the estimate coming from the different Authors estimates. Is good that is high, since it means that we have results exploring well the actual population estimate.

Within variance = is the part of the variance that comes from the same Type of dimensions (positive, negative or general belief).

```{r}

```



```{r}
df_meta_r = df_meta[df_meta$d_r == "r",]

df_meta_r$Summary_dim

table(as.character(df_meta_r$Authors), df_meta_r$Summary_dim)
```


This tells us that the studies present different results for the papers of the, but there is a reduced difference in the estimates from a paper that reports more than one effect size

```{r}
100- sum(Separated_I_squared)
```
This is the remaining due to sampling variance.



```{r}
calculate_estimates <- function(result_r1) {
  # Calculate W
  W <- diag(1/result_r1$vi)
  
  # Calculate X
  X <- model.matrix(result_r1)
  
  # Calculate P
  P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
  
  # Calculate I_square
  I_square = 100 * sum(result_r1$sigma2) / (sum(result_r1$sigma2) + (result_r1$k-result_r1$p)/sum(diag(P)))
  
  # Calculate Separated_I_squared
  Separated_I_squared = 100 * result_r1$sigma2 / (sum(result_r1$sigma2) + (result_r1$k-result_r1$p)/sum(diag(P)))
  
  # Name the elements of Separated_I_squared
  names(Separated_I_squared) = c("between_variance", "within_variance")
  
  # Calculate remaining_sampling_variance
  remaining_sampling_variance = 100 - sum(Separated_I_squared)
  
  # Return the results as a list
  return(list(I_square = I_square, Separated_I_squared = Separated_I_squared, remaining_sampling_variance = remaining_sampling_variance))
}


calculate_estimates(result_r1)
```
Model2 considering moderators:

```{r}
df_meta_r = escalc(measure = "ZCOR", ri = effect_size, ni = N,
                   data = df_meta_r, slab = Authors)

result_r2 =  rma.mv(yi,vi,
                    mods = ~ Age + Clinical + Instrument_simpl,
                    slab = Authors,
                    data = df_meta_r,
                    random = ~ 1 | Authors/Summary_dim,
                    test = "t", 
                    method = "REML")

```

```{r}
summary(result_r2)
```

The moderators are non in the complexity significant (see the F test, with p_value = $0.08 > 0.05$). It is worth it trying removing them from the equation and see if we can get something significant (Model selection)


```{r}
calculate_estimates(result_r2)
```


```{r}

result_r3 =  rma.mv(yi,vi,
                    mods = ~ Age + Clinical,
                    slab = Authors,
                    data = df_meta_r,
                    random = ~ 1 | Authors/Summary_dim,
                    test = "t", 
                    method = "REML")

summary(result_r3)
```

The Test for Residual Heterogeneity (QE) is significant (p < .0001), indicating that there is significant variation in effect sizes across studies that is not explained by the model.

The Test of Moderators is not significant (p = 0.0698), suggesting that the moderators included in the model (coefficients 2 and 3) do not significantly explain the variation in effect sizes across studies.


Here in such model the moderators are almost significant. Moreover, we notice that the redundant predictor in the warning is no more here. There is a problem with the other variable

```{r}
calculate_estimates(result_r3)
```



```{r}
result_r4 =  rma.mv(yi,vi,
                    mods = ~ Age + Instrument_simpl,
                    slab = Authors,
                    data = df_meta_r,
                    random = ~ 1 | Authors/Summary_dim,
                    test = "t", 
                    method = "REML")

# do not include this, is wrong!!
```

```{r}
plot(Age ~ Instrument_simpl, data = df_meta_r)
table(df_meta_r$Age, as.character(df_meta_r$Instrument_simpl))
```


```{r}
result_r5 = rma.mv(yi,vi,
                    mods = ~ Age,
                    slab = Authors,
                    data = df_meta_r,
                    random = ~ 1 | Authors/Summary_dim,
                    test = "t", 
                    method = "REML")
```

```{r}
AIC(result_r1, result_r2, result_r3, result_r4, result_r5)
```







### Converting d to r

The limitation of this is that the papers probably do not report the number of each groups

Finding the authors and checking the papers:
```{r}
d_means =  which(df_meta$d_r == "d")
d_authors = df_meta$Authors[d_means]
d_authors |> unique()
```

```{r}
library(esc)

d_V = df_meta$V[d_means]
d_measures = df_meta$effect_size[d_means]
d_n = df_meta$N[d_means]

d_n1 = floor(d_n/2) # assuming that the two groups have the same size
d_n2 = floor(d_n/2)

#d_authors = df_meta$Authors[d_means]
#d_authors |> unique()

r_measures = numeric(length(d_measures))
r_V  = numeric(length(d_measures))
r_SE = numeric(length(d_measures))

for(i in 1:length(d_measures)){
  result = convert_d2r(d = d_measures[i], v = d_V[i], grp1n = d_n1[i], grp2n = d_n2[i])
  r_measures[i] = result$es
  r_V[i] = result$var
  r_SE[i] = result$se
}

print(r_measures)
df_meta$effect_size[d_means] = r_measures
print(r_V)
df_meta$V[d_means] = r_V
print(r_SE)
df_meta$SE[d_means] = r_SE
#convert_d2r(d = d_measures, v = d_V, grp1n = d_n1, grp2n = d_n2)
#convert_d2r(d = d_measures, v = d_V )
```
Standardize the measure:
```{r}
df_meta_r = escalc(measure = "ZCOR", ri = effect_size, sei = SE,
                   data = df_meta, slab = Authors)
```

```{r}
?escalc
```

```{r}
res_r = rma(effect_size, N, data = df_meta_r)
res_r
```
```{r}
forest(res_r)
```





### Convert the r measurements into d measurements

```{r}
library(esc)
#convert_z2r(0.52)


df_meta
convert_r_to_d = function(r){
  num = 2*r
  denom = sqrt((1 - r^2))
  return(num/denom)
}

convertSE_r_toSE_d = function(SE, r){
  num = 4*sqrt(SE)
  denom = (1 - r^2)^3
  return(sqrt(num/denom))
}

corrs = which(df_meta$d_r == "r")
df_meta$SE[corrs] = convertSE_r_toSE_d(df_meta$SE[corrs], df_meta$effect_size[corrs])
df_meta$effect_size[corrs] = convert_r_to_d(df_meta$effect_size[corrs])
df_meta$d_r[corrs] = "d"
rm(corrs)
df_meta
```

### Check for outliers
```{r}
df_meta$effect_size
df_meta$SE

# Find the maximum value in the SE column
max_se_value <- which.max(df_meta$SE)
df_meta[max_se_value,]

# Remove rows with the maximum SE value
df_meta <- df_meta[-max_se_value, ]
rm(max_se_value)
```

Converting our effect sizes into the SMD, need to check that this is indeed true

```{r}
# Compute the standardized mean difference and its variance
df_meta = escalc(measure = "SMD", n1i = N, n2i = N,
                 yi = effect_size, sei = SE, slab = Authors, 
                 data = df_meta)
```

```{r}
print("Original variances:")
df_meta$V

print("Converted Variances:")
df_meta$vi # notice that the variances have been changed only for the r measures
```
## Multilevel model
```{r}
##### option 1: summary dim is nested in authors
full.model <- rma.mv(yi,
                     vi, 
                     mods = ~ Age + Clinical + Instrument_simpl,
                     slab = Authors,
                     data = df_meta,
                     random = ~ 1 | Authors/Summary_dim, # level 2: individual effect sizes come from the Summary dim. Nested within level 3, the studies
                     test = "t", 
                     method = "REML")
summary(full.model)

##### option 2: We don't even summarize, we just use the original dimensions 
full.model <- rma.mv(yi,
                     vi, 
                     mods = ~ Age + Clinical + Instrument_simpl,
                     slab = Authors,
                     data = df_meta,
                     random = ~ 1 | Authors/Dimensions,
                     test = "t", 
                     method = "REML")
summary(full.model)

```
### check out aggregation options
```{r}
library("dplyr")
detach("package:dplyr", unload = TRUE) # for some reason metafor::aggregate isn't working

##### option one, aggregate general, positive, and negative measures together (by taking the summary_dim column, which encodes other measures of metacognition as general mesuers). 
#     If a general measure of metacognition is present, it is averaged with the other sub measures.  maybe problematic
agg_meta = data.frame()
for(ID in unique(df_meta$Authors)){
  agg_measures <- aggregate(subset(df_meta, Authors == ID), cluster = Summary_dim, rho = 0.6) ## I picked rho arbitrarily 
  agg_meta = rbind(agg_meta, agg_measures)
}

full.model <- rma.mv(yi,
                     vi, 
                     mods = ~ Age + Clinical + Instrument_simpl,
                     slab = Authors,
                     data = agg_meta,
                     random = ~ 1 | Authors/Summary_dim,
                     test = "t", 
                     method = "REML")
summary(full.model)


##### option two, take only the meta measure if one is recorded, delete the rest. aggregate the positive and negative measures per author
agg_meta = data.frame()
for(ID in unique(df_meta$Authors)){ # for each of the studies
  subs = subset(df_meta, Authors == ID) # take a look at the studies one at a time
  if("General" %in% subs$Dimensions){ # If the study already reports a general measure
    row_rm = which(!(subs$Dimensions %in% c("General","positive dimension", "negative dimension"))) # find measures which aren't positive, negative, or general
    if (length(row_rm) > 0) { #If there are any
      subs <- subs[-row_rm, ] # delete them
    }
  }else{ # If no general measures are recorded in the study 
    subs_general = aggregate(subs, cluster = Authors, rho = 0.6) # create a general measure by aggregateing all others. This means information is repeated!
    subs_general$Dimensions = subs_general$Summary_dim = "General" # recode as general. If there was only one negative measure, that is repeated as a general measure! Check out Yu et al
    subs = rbind(subs_general,subs) # concatenate with the original positive and negative dimensions
  }
  
  agg_measures <- aggregate(subs, cluster = Summary_dim, rho = 0.6) # aggregate the repeated positive measures and the repeated negative measures
  agg_meta = rbind(agg_meta, agg_measures) # add author data together
}

full.model <- rma.mv(yi,
                     vi, 
                     mods = ~ Age + Clinical + Instrument_simpl,
                     slab = Authors,
                     data = agg_meta,
                     random = ~ 1 | Authors/Summary_dim,
                     test = "t", 
                     method = "REML")
summary(full.model)

```


```{r}
par(mar=c(4,4,2,2))
forest(full.model, header = T, mlab = "Summary",
       cex = 0.5)
```
```{r}
funnel(full.model)
```
